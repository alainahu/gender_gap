LLM: ChatGPT-4.
Date: 2 January 2024


User
Can you make a generic fun background rectangular image with the only text being "STA302"
ChatGPT

Generated by DALL·E
Here is a fun and engaging background image featuring the text "STA302". The design incorporates colorful and dynamic elements suitable for the theme.



User
how can i quickly explain gender parity?
ChatGPT
Gender parity refers to the equal representation and treatment of all genders, aiming for fairness in opportunities, rights, and benefits. It's often discussed in contexts such as education, employment, and leadership, where achieving gender parity means having an equitable distribution of genders in these areas. A quick way to explain it is to say that gender parity is achieved when people of all genders have the same opportunities and are represented equally in various sectors of society, without any gender being advantaged or disadvantaged.

User
how do i sort a tibble alphabetically by one column?
ChatGPT
To sort a tibble alphabetically by one column in R, you can use the arrange() function from the dplyr package. Here's a step-by-step guide on how to do it:

Load the dplyr package: If you haven't already installed dplyr, you can do so using install.packages("dplyr"). Then, load the package with library(dplyr).
Use the arrange() function: To sort the tibble alphabetically by a specific column, use the arrange() function and specify the tibble and the column by which you want to sort. For example, if you have a tibble named my_tibble and you want to sort it by the column column_name, you would do:
R
Copy code
library(dplyr)

sorted_tibble <- arrange(my_tibble, column_name)
If you want to sort in descending order, you can use desc() within the arrange() function:
R
Copy code
sorted_tibble <- arrange(my_tibble, desc(column_name))
Check the sorted tibble: You can view the sorted tibble by printing sorted_tibble.
Here’s a quick example:

R
Copy code
# Load the dplyr package
library(dplyr)

# Example tibble
my_tibble <- tibble(
  name = c("Alice", "Charlie", "Bob"),
  age = c(25, 30, 22)
)

# Sort by name in alphabetical order
sorted_tibble <- arrange(my_tibble, name)

# Print the sorted tibble
print(sorted_tibble)
This will output a tibble sorted alphabetically by the names:

perl
Copy code
# A tibble: 3 x 2
  name    age
  <chr> <dbl>
1 Alice    25
2 Bob      22
3 Charlie  30
Using arrange() is a straightforward way to sort data in a tibble.

User
how do i round all values in a column to two decimal places?
ChatGPT
To round all values in a column of a tibble to two decimal places in R, you can use the mutate() function from the dplyr package along with the base R round() function. Here's how you can do it:

Load the dplyr package: Make sure the dplyr package is loaded. If not, you can load it with library(dplyr).
Apply the round() function using mutate(): You can use mutate() to modify the column in question, applying the round() function to it. For example, if you have a tibble named my_tibble and you want to round the values in the column value to two decimal places, you would do:
R
Copy code
library(dplyr)

# Assuming my_tibble is already created and has a column named 'value'
my_tibble <- my_tibble %>%
  mutate(value = round(value, 2))
Check the modified tibble: You can view the updated tibble by printing my_tibble.
Here’s an example:

R
Copy code
# Load the dplyr package
library(dplyr)

# Example tibble
my_tibble <- tibble(
  name = c("Alice", "Charlie", "Bob"),
  value = c(25.123, 30.567, 22.89123)
)

# Round the 'value' column to two decimal places
my_tibble <- my_tibble %>%
  mutate(value = round(value, 2))

# Print the modified tibble
print(my_tibble)
This will output the tibble with the values in the value column rounded to two decimal places:

perl
Copy code
# A tibble: 3 x 2
  name    value
  <chr>  <dbl>
1 Alice   25.12
2 Charlie 30.57
3 Bob     22.89
This method efficiently rounds the values and updates the tibble.

User
i want to convert columns to a double
ChatGPT
To convert columns in a tibble to double type in R, you can use the mutate() function from the dplyr package along with the as.numeric() function, which effectively converts data types to double (since numeric in R is essentially double). Here’s how you can do it:

Load the dplyr package: First, make sure the dplyr package is loaded. If you haven't already installed dplyr, you can do so using install.packages("dplyr"), and then load the package with library(dplyr).
Use mutate() to convert column types: If you have a tibble named my_tibble and you want to convert specific columns to double, use the mutate() function. Suppose you want to convert the columns column1 and column2 to double, you would do:
R
Copy code
library(dplyr)

my_tibble <- my_tibble %>%
  mutate(column1 = as.numeric(column1),
         column2 = as.numeric(column2))
If you need to convert all columns of a certain type to double, you can use mutate_if():
R
Copy code
my_tibble <- my_tibble %>%
  mutate_if(is.integer, as.numeric)
This code converts all integer columns in the tibble to numeric (double).
Check the modified tibble: You can view the updated structure of your tibble to confirm the changes using str(my_tibble) or simply print the tibble.
Here’s a complete example demonstrating the process:

R
Copy code
# Load the dplyr package
library(dplyr)

# Example tibble
my_tibble <- tibble(
  name = c("Alice", "Charlie", "Bob"),  # character column
  integer_column = c(1L, 2L, 3L),       # integer column
  numeric_column = c(4.5, 5.6, 6.7)     # already numeric, but let's include it
)

# Convert the 'integer_column' to double
my_tibble <- my_tibble %>%
  mutate(integer_column = as.numeric(integer_column))

# Print the structure of the modified tibble
str(my_tibble)
This will convert the integer_column to double, maintaining the data integrity while changing the data type.

User
There was 1 warning in `mutate()`.
ℹ In argument: `GDP = as.numeric(GDP)`.
Caused by warning:
! NAs introduced by coercion
ChatGPT
The warning message "NAs introduced by coercion" typically occurs when trying to convert a column to numeric type using as.numeric() in R, and the column contains values that cannot be converted to numbers. This might happen if the column has mixed types, such as numeric values combined with characters or symbols that cannot be interpreted as numbers.

Here are a few steps you can take to address this issue:

Identify Non-Numeric Values: First, check the column for any non-numeric values that might be causing the conversion to fail. You can do this by examining unique entries that might not be numbers. Here’s how you can inspect these values:
R
Copy code
library(dplyr)

# Assuming my_tibble has a column named 'GDP'
non_numeric_values <- my_tibble %>%
  filter(!grepl("^[0-9\\.]+$", GDP)) %>%
  pull(GDP) %>%
  unique()

print(non_numeric_values)
This code filters out entries in the GDP column that do not match a regular expression for numeric values (including floating point numbers), and displays the unique non-numeric values.
Handle Non-Numeric Values: Once you've identified the problematic entries, you can decide how to handle them. Options include:
Removing non-numeric entries: If these entries are not critical, you can remove them.
Replacing with NA or a default value: If you prefer to keep the number of entries unchanged, replace non-numeric values with NA or another placeholder, like 0.
Correcting data entries: Sometimes, non-numeric values may be due to data entry errors that you can correct.
Convert Again: After cleaning or modifying the column, attempt the conversion again. For instance, if you decide to replace non-numeric entries with NA, here’s how you might do it:
R
Copy code
my_tibble <- my_tibble %>%
  mutate(GDP = as.numeric(as.character(GDP)))  # ensure conversion from factor if necessary

# Check for errors or warnings
sum(is.na(my_tibble$GDP))
Verify the Data: Finally, it’s good practice to verify the changes by checking the structure of your data and ensuring the GDP column is now properly formatted as numeric:
R
Copy code
str(my_tibble)
By following these steps, you should be able to convert your GDP column to numeric type without errors, and handle any non-numeric data effectively.

User
how many countries are here? Afghanistan	2021	355.8	
Albania	2022	6,810.1	
Algeria	2022	4,342.6	
American Samoa	2022	19,673.4	
Andorra	2022	41,992.8	
Angola	2022	3,000.4	
Antigua and Barbuda	2022	19,919.7	
Argentina	2022	13,650.6	
Armenia	2022	7,018.1	
Aruba	2022	33,300.8	
Australia	2022	65,099.8	
Austria	2022	52,084.7	
Azerbaijan	2022	7,762.1	
Bahamas, The	2022	31,458.3	
Bahrain	2022	30,146.9	
Bangladesh	2022	2,688.3	
Barbados	2022	20,238.8	
Belarus	2022	7,888.3	
Belgium	2022	49,926.8	
Belize	2022	6,984.2	
Benin	2022	1,302.9	
Bermuda	2022	118,774.8	
Bhutan	2021	3,560.2	
Bolivia	2022	3,600.1	
Bosnia and Herzegovina	2022	7,568.8	
Botswana	2022	7,738.9	
Brazil	2022	8,917.7	
British Virgin Islands			
Brunei Darussalam	2022	37,152.5	
Bulgaria	2022	13,974.4	
Burkina Faso	2022	830.0	
Burundi	2022	259.0	
Cabo Verde	2022	3,754.3	
Cambodia	2022	1,759.6	
Cameroon	2022	1,563.5	
Canada	2022	55,522.4	
Cayman Islands	2022	99,624.9	
Central African Republic	2022	427.1	
Chad	2022	716.8	
Channel Islands	2021	67,960.7	
Chile	2022	15,355.5	
China	2022	12,720.2	
Colombia	2022	6,624.2	
Comoros	2022	1,484.9	
Congo, Dem. Rep.	2022	653.7	
Congo, Rep.	2022	2,649.2	
Costa Rica	2022	13,365.4	
Cote d'Ivoire	2022	2,486.4	
Croatia	2022	18,570.4	
Cuba	2020	9,499.6	
Curacao	2022	20,501.8	
Cyprus	2022	32,048.2	
Czechia	2022	27,226.6	
Denmark	2022	67,790.1	
Djibouti	2022	3,136.1	
Dominica	2022	8,351.2	
Dominican Republic	2022	10,111.2	
Ecuador	2022	6,391.3	
Egypt, Arab Rep.	2022	4,295.4	
El Salvador	2022	5,127.3	
Equatorial Guinea	2022	7,182.3	
Eritrea	2011	643.8	
Estonia	2022	28,247.1	
Eswatini	2022	3,986.9	
Ethiopia	2022	1,027.6	
Faroe Islands	2022	66,979.3	
Fiji	2022	5,356.2	
Finland	2022	50,871.9	
France	2022	40,886.3	
French Polynesia	2022	18,984.9	
Gabon	2022	8,820.3	
Gambia, The	2022	808.3	
Georgia	2022	6,675.0	
Germany	2022	48,718.0	
Ghana	2022	2,203.6	
Gibraltar			
Greece	2022	20,867.3	
Greenland	2021	57,116.3	
Grenada	2022	9,689.1	
Guam	2021	35,904.9	
Guatemala	2022	5,473.2	
Guinea	2022	1,515.2	
Guinea-Bissau	2022	775.8	
Guyana	2022	18,199.5	
Haiti	2022	1,748.3	
Honduras	2022	3,040.2	
Hong Kong SAR, China	2022	48,983.6	
Hungary	2022	18,390.2	
Iceland	2022	73,466.8	
India	2022	2,410.9	
Indonesia	2022	4,788.0	
Iran, Islamic Rep.	2022	4,669.6	
Iraq	2022	5,937.2	
Ireland	2022	103,983.3	
Isle of Man	2020	79,530.6	
Israel	2022	54,930.9	
Italy	2022	34,776.4	
Jamaica	2022	6,047.2	
Japan	2022	34,017.3	
Jordan	2022	4,311.0	
Kazakhstan	2022	11,492.0	
Kenya	2022	2,099.3	
Kiribati	2022	1,702.0	
Korea, Dem. People's Rep.			
Korea, Rep.	2022	32,422.6	
Kosovo	2022	5,340.3	
Kuwait	2022	41,079.5	
Kyrgyz Republic	2022	1,655.1	
Lao PDR	2022	2,054.4	
Latvia	2022	21,779.5	
Lebanon	2021	4,136.1	
Lesotho	2022	969.9	
Liberia	2022	754.5	
Libya	2022	6,716.1	
Liechtenstein	2021	197,504.5	
Lithuania	2022	25,064.8	
Luxembourg	2022	125,006.0	
Macao SAR, China	2022	34,584.9	
Madagascar	2022	516.6	
Malawi	2022	645.2	
Malaysia	2022	11,993.2	
Maldives	2022	11,780.8	
Mali	2022	833.3	
Malta	2022	34,127.5	
Marshall Islands	2022	6,225.2	
Mauritania	2022	2,065.2	
Mauritius	2022	10,256.2	
Mexico	2022	11,496.5	
Micronesia, Fed. Sts.	2022	3,714.0	
Moldova	2022	5,714.4	
Monaco	2022	240,862.2	
Mongolia	2022	5,045.5	
Montenegro	2022	10,093.4	
Morocco	2022	3,442.0	
Mozambique	2022	558.3	
Myanmar	2022	1,149.2	
Namibia	2022	5,031.1	
Nauru	2022	11,970.9	
Nepal	2022	1,336.5	
Netherlands	2022	57,025.0	
New Caledonia	2022	35,745.2	
New Zealand	2022	48,418.6	
Nicaragua	2022	2,255.4	
Niger	2022	585.4	
Nigeria	2022	2,162.6	
North Macedonia	2022	6,591.5	
Northern Mariana Islands	2020	17,302.9	
Norway	2022	108,729.2	
Oman	2022	25,056.8	
Pakistan	2022	1,588.9	
Palau	2021	12,921.8	
Panama	2022	17,357.6	
Papua New Guinea	2022	3,115.9	
Paraguay	2022	6,153.1	
Peru	2022	7,125.8	
Philippines	2022	3,498.5	
Poland	2022	18,688.0	
Portugal	2022	24,515.3	
Puerto Rico	2022	35,208.6	
Qatar	2022	87,661.5	
Romania	2022	15,786.8	
Russian Federation	2022	15,270.7	
Rwanda	2022	966.2	
Samoa	2022	3,745.6	
San Marino	2021	54,982.5	
Sao Tome and Principe	2022	2,386.7	
Saudi Arabia	2022	30,447.9	
Senegal	2022	1,598.7	
Serbia	2022	9,537.7	
Seychelles	2022	13,250.5	
Sierra Leone	2022	475.8	
Singapore	2022	82,807.6	
Sint Maarten (Dutch part)	2022	35,873.1	
Slovak Republic	2022	21,256.8	
Slovenia	2022	28,439.3	
Solomon Islands	2022	2,205.3	
Somalia	2022	592.1	
South Africa	2022	6,766.5	
South Sudan	2015	1,071.8	
Spain	2022	29,674.5	
Sri Lanka	2022	3,354.4	
St. Kitts and Nevis	2022	20,262.3	
St. Lucia	2022	13,030.9	
St. Martin (French part)	2021	20,320.7	
St. Vincent and the Grenadines	2022	9,125.3	
Sudan	2022	1,102.1	
Suriname	2022	5,858.8	
Sweden	2022	56,424.3	
Switzerland	2022	93,259.9	
Syrian Arab Republic	2021	420.6	
Tajikistan	2022	1,054.2	
Tanzania	2022	1,192.8	
Thailand	2022	6,910.0	
Timor-Leste	2022	2,389.3	
Togo	2022	942.6	
Tonga	2021	4,426.0	
Trinidad and Tobago	2022	19,629.5	
Tunisia	2022	3,747.4	
Turkiye	2022	10,674.5	
Turkmenistan	2022	8,792.5	
Turks and Caicos Islands	2022	24,917.6	
Tuvalu	2022	5,221.5	
Uganda	2022	964.4	
Ukraine	2022	4,534.0	
United Arab Emirates	2022	53,708.0	
United Kingdom	2022	46,125.3	
United States	2022	76,329.6	
Uruguay	2022	20,795.0	
Uzbekistan	2022	2,255.2	
Vanuatu	2022	3,231.4	
Venezuela, RB	2014	15,975.7	
Viet Nam	2022	4,163.5	
Virgin Islands (U.S.)	2021	41,976.0	
West Bank and Gaza	2022	3,789.3	
Yemen, Rep.	2022	650.3	
Zambia	2022	1,456.9	
Zimbabwe
ChatGPT
To count the number of countries listed in the data you provided, we can treat each line as a separate entry, assuming each line represents a different country or territory. I will count the lines to determine the number of countries mentioned.

Let's count them now.

Error analyzing
It seems there was an issue processing the request due to the large input size. Instead, I'll adjust my approach and manually count the lines of text you provided, where each line represents a country or region, to determine how many are listed. Let me redo that correctly. ​​

Error analyzing
It seems I encountered another issue with the large amount of text. Let's streamline the process. I'll count the lines directly, which represent each country or territory from the snippet of data you've shown. I will then provide the total count. Let's count them now. ​

User
Through the plot, we observe a positive relationship between attitudes justifying intimate-partner violence and the gender gap across the countries. This means that in countries where a higher percentage of women justify violence from their partners, there is an association with larger gender gaps. 
can you clarify and better word this
ChatGPT
The relationship observed in the data indicates that countries where a higher percentage of women accept intimate-partner violence as justified tend to exhibit wider disparities between genders. This suggests that societal norms that condone violence against women are associated with greater gender inequality.

User
im using prior summary in stan glm.  > attitude_data <- read_csv("data/raw_data/raw_data.csv")
Rows: 14951 Columns: 11                                                                                                                      
── Column specification ──────────────────────────────────────────────────────────────────────────────────
Delimiter: ","
chr (6): REGION, Region, LOCATION, Country, VAR, Variables
dbl (3): TIME, Time, Value
lgl (2): Flag Codes, Flags

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> gendergap_data <- read_excel("data/raw_data/global-gender-gap-index-2023.xlsx")
New names:                                                                                              
• `` -> `...2`
> 
> gdp_data <- read_csv("data/raw_data/gdp_data.csv")
Rows: 271 Columns: 5                                                                                                                         
── Column specification ──────────────────────────────────────────────────────────────────────────────────
Delimiter: ","
chr (5): Series Name, Series Code, Country Name, Country Code, 2022 [YR2022]

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> 
> 
> 
> 
> attitude_data <-
+   attitude_data |>
+   select(Region, LOCATION, Country, Variables, Value) |>
+   filter(Region == "All regions") |> 
+   filter(Variables %in%  c("Attitudes on women's income", "Attitudes justifying intimate-partner violence", "Attitudes on women's ability to be a political leader")) |>
+   mutate(Value = as.numeric(Value)) |>
+   select(-Region, -Variables) |>
+   rename(attitude = Value) 
> 
> attitude_data <- attitude_data |>
+   group_by(Country) |>
+   mutate(attitude_id = row_number()) |>
+   ungroup()
> 
> attitude_data <- attitude_data |>
+   pivot_wider(
+     names_from = attitude_id,
+     values_from = attitude,
+     names_prefix = "attitude_"
+     )
> 
> 
> attitude_data <- attitude_data |>
+   drop_na(attitude_data) |>
+   rename(
+     `Income Attitude` = attitude_1,
+     `Violence Attitude` = attitude_2,
+     `Political Attitude` = attitude_3
+   )
Error in `drop_na()`:
! Can't subset columns with `attitude_data`.
✖ `attitude_data` must be numeric or character, not a <tbl_df/tbl/data.frame> object.
Run `rlang::last_trace()` to see where the error occurred.
Warning message:
Using an external vector in selections was deprecated in tidyselect 1.1.0.
ℹ Please use `all_of()` or `any_of()` instead.
  # Was:
  data %>% select(attitude_data)

  # Now:
  data %>% select(all_of(attitude_data))

See <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.
This warning is displayed once every 8 hours.
Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. 
> library(tidyverse)
> library(readxl)
> 
> #### Clean data ####
> attitude_data <- read_csv("data/raw_data/raw_data.csv")
Rows: 14951 Columns: 11                                                                                                                      
── Column specification ──────────────────────────────────────────────────────────────────────────────────
Delimiter: ","
chr (6): REGION, Region, LOCATION, Country, VAR, Variables
dbl (3): TIME, Time, Value
lgl (2): Flag Codes, Flags

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> gendergap_data <- read_excel("data/raw_data/global-gender-gap-index-2023.xlsx")
New names:                                                                                              
• `` -> `...2`
> 
> gdp_data <- read_csv("data/raw_data/gdp_data.csv")
Rows: 271 Columns: 5                                                                                                                         
── Column specification ──────────────────────────────────────────────────────────────────────────────────
Delimiter: ","
chr (5): Series Name, Series Code, Country Name, Country Code, 2022 [YR2022]

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> 
> 
> 
> 
> attitude_data <-
+   attitude_data |>
+   select(Region, LOCATION, Country, Variables, Value) |>
+   filter(Region == "All regions") |> 
+   filter(Variables %in%  c("Attitudes on women's income", "Attitudes justifying intimate-partner violence", "Attitudes on women's ability to be a political leader")) |>
+   mutate(Value = as.numeric(Value)) |>
+   select(-Region, -Variables) |>
+   rename(attitude = Value) 
> 
> attitude_data <- attitude_data |>
+   group_by(Country) |>
+   mutate(attitude_id = row_number()) |>
+   ungroup()
> 
> attitude_data <- attitude_data |>
+   pivot_wider(
+     names_from = attitude_id,
+     values_from = attitude,
+     names_prefix = "attitude_"
+     )
> 
> 
> attitude_data <- attitude_data |>
+   rename(
+     `Income Attitude` = attitude_1,
+     `Violence Attitude` = attitude_2,
+     `Political Attitude` = attitude_3
+   )
> 
> 
> column_names <- names(gendergap_data)
> column_names[1] <- "Country"
> column_names[2] <- "Gender Gap"
> names(gendergap_data) <- column_names
> 
> 
> column_names_1 <- names(gdp_data)
> column_names_1 <- c("Series", "Series Code", "Country", "Code", "GDP")
> names(gdp_data) <- column_names_1
> 
> gdp_data <-
+   gdp_data |>
+   select(Country, Code, GDP)
> 
> 
> merged_data <- left_join(attitude_data, gdp_data, by = "Country")  
> merged_data <- left_join(merged_data, gendergap_data, by = "Country")
> merged_data <- merged_data |>
+   select(-LOCATION) |>
+   drop_na()
> 
> 
> 
> 
> 
> #### Save data ####
> write_csv(merged_data, "data/analysis_data.csv")
                                                                                                        
> View(merged_data)
> merged_data <- arrange(merged_data, Country)
> View(merged_data)
> merged_data <- merged_data |>
+   mutate(GDP = round(GDP, 2))
Error in `mutate()`:
ℹ In argument: `GDP = round(GDP, 2)`.
Caused by error in `round()`:
! non-numeric argument to mathematical function
Run `rlang::last_trace()` to see where the error occurred.
> merged_data <- merged_data |>
+   mutate(`Income Attitude` = as.numeric(`Income Attitude`),
+          `Violence Attitude`= as.numeric(`Violence Attitude`),
+          `Political Attitude` = as.numeric(`Political Attitude`),
+          GDP = as.numeric(GDP),
+          `Gender Gap` = as.numeric(`Gender Gap`)
+          )
Warning message:
There was 1 warning in `mutate()`.
ℹ In argument: `GDP = as.numeric(GDP)`.
Caused by warning:
! NAs introduced by coercion 
> View(merged_data)
> merged_data <- merged_data |>
+   mutate(GDP = round(GDP, 2))
> View(merged_data)
> data_path <- here("data", "analysis_data.csv")
Error in here("data", "analysis_data.csv") : 
  could not find function "here"
> library(here)
> data_path <- here("data", "analysis_data.csv")
> data <- read_csv(data_path, show_col_types = FALSE)
> data |>
+   select(-Code) |>
+   slice(1:5) |>
+   kable(align = c('c', 'c', 'c', 'c')) |>
+   kable_styling(latex_options = c("hold_position"))
Error in kable_styling(kable(slice(select(data, -Code), 1:5), align = c("c",  : 
  could not find function "kable_styling"
> library(kableExtra)
> 
> data |>
+   select(-Code) |>
+   slice(1:5) |>
+   kable(align = c('c', 'c', 'c', 'c')) |>
+   kable_styling(latex_options = c("hold_position"))
> #### Save data ####
> write_csv(merged_data, "data/analysis_data.csv")
                                                                                                        
> #| label: tbl-cleaned_data
> #| tbl-cap: Sample of cleaned attitude, GDP, and gender gap data
> #| echo: false
> 
> data_path <- here("data", "analysis_data.csv")
> data <- read_csv(data_path, show_col_types = FALSE)
> 
> data |>
+   select(-Code) |>
+   slice(1:5) |>
+   kable(align = c('c', 'c', 'c', 'c')) |>
+   kable_styling(latex_options = c("hold_position"))
>   
> #| tbl-cap: "Summary Statistics"
> #| label: tbl-summ-stats
> #| echo: false
> 
> summary <- data |>
+   summarise(
+     Mean_Income = mean(`Income Attitude`, na.rm = TRUE),
+     Median_Income = median(`Income Attitude`, na.rm = TRUE),
+     SD_Income = sd(`Income Attitude`, na.rm = TRUE),
+     Min_Income = min(`Income Attitude`, na.rm = TRUE),
+     Max_Income = max(`Income Attitude`, na.rm = TRUE),
+     Mean_Violence = mean(`Violence Attitude`, na.rm = TRUE),
+     Median_Violence = median(`Violence Attitude`, na.rm = TRUE),
+     SD_Violence = sd(`Violence Attitude`, na.rm = TRUE),
+     Min_Violence = min(`Violence Attitude`, na.rm = TRUE),
+     Max_Violence = max(`Violence Attitude`, na.rm = TRUE),
+     Mean_Political = mean(`Political Attitude`, na.rm = TRUE),
+     Median_Political = median(`Political Attitude`, na.rm = TRUE),
+     SD_Political = sd(`Political Attitude`, na.rm = TRUE),
+     Min_Political = min(`Political Attitude`, na.rm = TRUE),
+     Max_Political = max(`Political Attitude`, na.rm = TRUE),
+     Mean_GDP = mean(GDP, na.rm = TRUE),
+     Median_GDP = median(GDP, na.rm = TRUE),
+     SD_GDP = sd(GDP, na.rm = TRUE),
+     Min_GDP = min(GDP, na.rm = TRUE),
+     Max_GDP = max(GDP, na.rm = TRUE),
+     Mean_Gap = mean(`Gender Gap`, na.rm = TRUE),
+     Median_Gap = median(`Gender Gap`, na.rm = TRUE),
+     SD_Gap = sd(`Gender Gap`, na.rm = TRUE),
+     Min_Gap = min(`Gender Gap`, na.rm = TRUE),
+     Max_Gap = max(`Gender Gap`, na.rm = TRUE)
+   )
> 
> long_summary_stats <-  summary |>
+   pivot_longer(cols = everything(), names_to = "Statistic", values_to = "Value") |>
+   separate(Statistic, into = c("Measure", "Variable"), sep = "_") |>
+   spread(key = Measure, value = Value) |>
+   select(Variable, Mean, Median, SD, Min, Max)
> 
> kable(long_summary_stats,
+       format = "markdown",
+       digits = 2,
+       col.names = c("Variable", "Mean", "Median", "Standard. Deviation", "Min", "Max"),
+       row.names = FALSE,
+       align = c('l', 'c', 'c', 'c', 'c', 'c'), 
+       )
> #| tbl-cap: "Summary Statistics"
> #| label: tbl-summ-stats
> #| echo: false
> 
> summary <- data |>
+   summarise(
+     Mean_Income = mean(`Income Attitude`, na.rm = TRUE),
+     Median_Income = median(`Income Attitude`, na.rm = TRUE),
+     SD_Income = sd(`Income Attitude`, na.rm = TRUE),
+     Min_Income = min(`Income Attitude`, na.rm = TRUE),
+     Max_Income = max(`Income Attitude`, na.rm = TRUE),
+     Mean_Violence = mean(`Violence Attitude`, na.rm = TRUE),
+     Median_Violence = median(`Violence Attitude`, na.rm = TRUE),
+     SD_Violence = sd(`Violence Attitude`, na.rm = TRUE),
+     Min_Violence = min(`Violence Attitude`, na.rm = TRUE),
+     Max_Violence = max(`Violence Attitude`, na.rm = TRUE),
+     Mean_Political = mean(`Political Attitude`, na.rm = TRUE),
+     Median_Political = median(`Political Attitude`, na.rm = TRUE),
+     SD_Political = sd(`Political Attitude`, na.rm = TRUE),
+     Min_Political = min(`Political Attitude`, na.rm = TRUE),
+     Max_Political = max(`Political Attitude`, na.rm = TRUE),
+     Mean_GDP = mean(GDP, na.rm = TRUE),
+     Median_GDP = median(GDP, na.rm = TRUE),
+     SD_GDP = sd(GDP, na.rm = TRUE),
+     Min_GDP = min(GDP, na.rm = TRUE),
+     Max_GDP = max(GDP, na.rm = TRUE),
+     Mean_Gap = mean(`Gender Gap`, na.rm = TRUE),
+     Median_Gap = median(`Gender Gap`, na.rm = TRUE),
+     SD_Gap = sd(`Gender Gap`, na.rm = TRUE),
+     Min_Gap = min(`Gender Gap`, na.rm = TRUE),
+     Max_Gap = max(`Gender Gap`, na.rm = TRUE)
+   )
> 
> long_summary_stats <-  summary |>
+   pivot_longer(cols = everything(), names_to = "Statistic", values_to = "Value") |>
+   separate(Statistic, into = c("Measure", "Variable"), sep = "_") |>
+   spread(key = Measure, value = Value) |>
+   select(Variable, Mean, Median, SD, Min, Max)
> 
> long_summary_stats$Variable <- c("Income Attitude", "Violence Attitude", "Political Attitude", "GDP", "Gender Gap")
> kable(long_summary_stats,
+       format = "markdown",
+       digits = 2,
+       col.names = c("Variable", "Mean", "Median", "Standard. Deviation", "Min", "Max"),
+       row.names = FALSE,
+       align = c('l', 'c', 'c', 'c', 'c', 'c'), 
+       )
> #| tbl-cap: "Summary Statistics"
> #| label: tbl-summ-stats
> #| echo: false
> 
> summary <- data |>
+   summarise(
+     Mean_Income = mean(`Income Attitude`, na.rm = TRUE),
+     Median_Income = median(`Income Attitude`, na.rm = TRUE),
+     SD_Income = sd(`Income Attitude`, na.rm = TRUE),
+     Min_Income = min(`Income Attitude`, na.rm = TRUE),
+     Max_Income = max(`Income Attitude`, na.rm = TRUE),
+     Mean_Violence = mean(`Violence Attitude`, na.rm = TRUE),
+     Median_Violence = median(`Violence Attitude`, na.rm = TRUE),
+     SD_Violence = sd(`Violence Attitude`, na.rm = TRUE),
+     Min_Violence = min(`Violence Attitude`, na.rm = TRUE),
+     Max_Violence = max(`Violence Attitude`, na.rm = TRUE),
+     Mean_Political = mean(`Political Attitude`, na.rm = TRUE),
+     Median_Political = median(`Political Attitude`, na.rm = TRUE),
+     SD_Political = sd(`Political Attitude`, na.rm = TRUE),
+     Min_Political = min(`Political Attitude`, na.rm = TRUE),
+     Max_Political = max(`Political Attitude`, na.rm = TRUE),
+     Mean_GDP = mean(GDP, na.rm = TRUE),
+     Median_GDP = median(GDP, na.rm = TRUE),
+     SD_GDP = sd(GDP, na.rm = TRUE),
+     Min_GDP = min(GDP, na.rm = TRUE),
+     Max_GDP = max(GDP, na.rm = TRUE),
+     Mean_Gap = mean(`Gender Gap`, na.rm = TRUE),
+     Median_Gap = median(`Gender Gap`, na.rm = TRUE),
+     SD_Gap = sd(`Gender Gap`, na.rm = TRUE),
+     Min_Gap = min(`Gender Gap`, na.rm = TRUE),
+     Max_Gap = max(`Gender Gap`, na.rm = TRUE)
+   )
> 
> long_summary_stats <-  summary |>
+   pivot_longer(cols = everything(), names_to = "Statistic", values_to = "Value") |>
+   separate(Statistic, into = c("Measure", "Variable"), sep = "_") |>
+   spread(key = Measure, value = Value) |>
+   select(Variable, Mean, Median, SD, Min, Max)
> 
> 
> 
> kable(long_summary_stats,
+       format = "markdown",
+       digits = 2,
+       col.names = c("Variable", "Mean", "Median", "Standard. Deviation", "Min", "Max"),
+       row.names = FALSE,
+       align = c('l', 'c', 'c', 'c', 'c', 'c'), 
+       )
> #| tbl-cap: "Summary Statistics"
> #| label: tbl-summ-stats
> #| echo: false
> 
> summary <- data |>
+   summarise(
+     Mean_Income = mean(`Income Attitude`, na.rm = TRUE),
+     Median_Income = median(`Income Attitude`, na.rm = TRUE),
+     SD_Income = sd(`Income Attitude`, na.rm = TRUE),
+     Min_Income = min(`Income Attitude`, na.rm = TRUE),
+     Max_Income = max(`Income Attitude`, na.rm = TRUE),
+     Mean_Violence = mean(`Violence Attitude`, na.rm = TRUE),
+     Median_Violence = median(`Violence Attitude`, na.rm = TRUE),
+     SD_Violence = sd(`Violence Attitude`, na.rm = TRUE),
+     Min_Violence = min(`Violence Attitude`, na.rm = TRUE),
+     Max_Violence = max(`Violence Attitude`, na.rm = TRUE),
+     Mean_Political = mean(`Political Attitude`, na.rm = TRUE),
+     Median_Political = median(`Political Attitude`, na.rm = TRUE),
+     SD_Political = sd(`Political Attitude`, na.rm = TRUE),
+     Min_Political = min(`Political Attitude`, na.rm = TRUE),
+     Max_Political = max(`Political Attitude`, na.rm = TRUE),
+     Mean_GDP = mean(GDP, na.rm = TRUE),
+     Median_GDP = median(GDP, na.rm = TRUE),
+     SD_GDP = sd(GDP, na.rm = TRUE),
+     Min_GDP = min(GDP, na.rm = TRUE),
+     Max_GDP = max(GDP, na.rm = TRUE),
+     Mean_Gap = mean(`Gender Gap`, na.rm = TRUE),
+     Median_Gap = median(`Gender Gap`, na.rm = TRUE),
+     SD_Gap = sd(`Gender Gap`, na.rm = TRUE),
+     Min_Gap = min(`Gender Gap`, na.rm = TRUE),
+     Max_Gap = max(`Gender Gap`, na.rm = TRUE)
+   )
> 
> long_summary_stats <-  summary |>
+   pivot_longer(cols = everything(), names_to = "Statistic", values_to = "Value") |>
+   separate(Statistic, into = c("Measure", "Variable"), sep = "_") |>
+   spread(key = Measure, value = Value) |>
+   select(Variable, Mean, Median, SD, Min, Max)
> 
> long_summary_stats$Variable <- c("Gender Gap", "GDP per capita", "Income Attitude", "Political Attitude", "Violence Attitude")
> 
> kable(long_summary_stats,
+       format = "markdown",
+       digits = 2,
+       col.names = c("Variable", "Mean", "Median", "Standard Deviation", "Min", "Max"),
+       row.names = FALSE,
+       align = c('l', 'c', 'c', 'c', 'c', 'c'), 
+       )
> #| tbl-cap: "Summary Statistics"
> #| label: tbl-summ-stats
> #| echo: false
> 
> summary <- data |>
+   summarise(
+     Mean_Income = mean(`Income Attitude`, na.rm = TRUE),
+     Median_Income = median(`Income Attitude`, na.rm = TRUE),
+     SD_Income = sd(`Income Attitude`, na.rm = TRUE),
+     Min_Income = min(`Income Attitude`, na.rm = TRUE),
+     Max_Income = max(`Income Attitude`, na.rm = TRUE),
+     Mean_Violence = mean(`Violence Attitude`, na.rm = TRUE),
+     Median_Violence = median(`Violence Attitude`, na.rm = TRUE),
+     SD_Violence = sd(`Violence Attitude`, na.rm = TRUE),
+     Min_Violence = min(`Violence Attitude`, na.rm = TRUE),
+     Max_Violence = max(`Violence Attitude`, na.rm = TRUE),
+     Mean_Political = mean(`Political Attitude`, na.rm = TRUE),
+     Median_Political = median(`Political Attitude`, na.rm = TRUE),
+     SD_Political = sd(`Political Attitude`, na.rm = TRUE),
+     Min_Political = min(`Political Attitude`, na.rm = TRUE),
+     Max_Political = max(`Political Attitude`, na.rm = TRUE),
+     Mean_GDP = mean(GDP, na.rm = TRUE),
+     Median_GDP = median(GDP, na.rm = TRUE),
+     SD_GDP = sd(GDP, na.rm = TRUE),
+     Min_GDP = min(GDP, na.rm = TRUE),
+     Max_GDP = max(GDP, na.rm = TRUE),
+     Mean_Gap = mean(`Gender Gap`, na.rm = TRUE),
+     Median_Gap = median(`Gender Gap`, na.rm = TRUE),
+     SD_Gap = sd(`Gender Gap`, na.rm = TRUE),
+     Min_Gap = min(`Gender Gap`, na.rm = TRUE),
+     Max_Gap = max(`Gender Gap`, na.rm = TRUE)
+   )
> 
> long_summary_stats <-  summary |>
+   pivot_longer(cols = everything(), names_to = "Statistic", values_to = "Value") |>
+   separate(Statistic, into = c("Measure", "Variable"), sep = "_") |>
+   spread(key = Measure, value = Value) |>
+   select(Variable, Mean, Median, SD, Min, Max)
> 
> long_summary_stats$Variable <- c("Gender Gap", "GDP", "Income Attitude", "Political Attitude", "Violence Attitude")
> 
> kable(long_summary_stats,
+       format = "markdown",
+       digits = 2,
+       col.names = c("Variable", "Mean", "Median", "Standard Deviation", "Min", "Max"),
+       row.names = FALSE,
+       align = c('l', 'c', 'c', 'c', 'c', 'c'), 
+       )
> merged_data <- merged_data |>
+   mutate(GDP = round(GDP, 2)) |>
+   mutate(`Gender Gap` = 1 - `Gender Gap`)
> View(merged_data)
> #### Save data ####
> write_csv(merged_data, "data/analysis_data.csv")
                                                                                                        
> #| label: tbl-cleaned_data
> #| tbl-cap: Sample of cleaned attitude, GDP, and gender gap data
> #| echo: false
> 
> data_path <- here("data", "analysis_data.csv")
> data <- read_csv(data_path, show_col_types = FALSE)
> 
> data |>
+   select(-Code) |>
+   slice(1:5) |>
+   kable(align = c('c', 'c', 'c', 'c')) |>
+   kable_styling(latex_options = c("hold_position"))
>   
> #| tbl-cap: "Summary Statistics"
> #| label: tbl-summ-stats
> #| echo: false
> 
> summary <- data |>
+   summarise(
+     Mean_Income = mean(`Income Attitude`, na.rm = TRUE),
+     Median_Income = median(`Income Attitude`, na.rm = TRUE),
+     SD_Income = sd(`Income Attitude`, na.rm = TRUE),
+     Min_Income = min(`Income Attitude`, na.rm = TRUE),
+     Max_Income = max(`Income Attitude`, na.rm = TRUE),
+     Mean_Violence = mean(`Violence Attitude`, na.rm = TRUE),
+     Median_Violence = median(`Violence Attitude`, na.rm = TRUE),
+     SD_Violence = sd(`Violence Attitude`, na.rm = TRUE),
+     Min_Violence = min(`Violence Attitude`, na.rm = TRUE),
+     Max_Violence = max(`Violence Attitude`, na.rm = TRUE),
+     Mean_Political = mean(`Political Attitude`, na.rm = TRUE),
+     Median_Political = median(`Political Attitude`, na.rm = TRUE),
+     SD_Political = sd(`Political Attitude`, na.rm = TRUE),
+     Min_Political = min(`Political Attitude`, na.rm = TRUE),
+     Max_Political = max(`Political Attitude`, na.rm = TRUE),
+     Mean_GDP = mean(GDP, na.rm = TRUE),
+     Median_GDP = median(GDP, na.rm = TRUE),
+     SD_GDP = sd(GDP, na.rm = TRUE),
+     Min_GDP = min(GDP, na.rm = TRUE),
+     Max_GDP = max(GDP, na.rm = TRUE),
+     Mean_Gap = mean(`Gender Gap`, na.rm = TRUE),
+     Median_Gap = median(`Gender Gap`, na.rm = TRUE),
+     SD_Gap = sd(`Gender Gap`, na.rm = TRUE),
+     Min_Gap = min(`Gender Gap`, na.rm = TRUE),
+     Max_Gap = max(`Gender Gap`, na.rm = TRUE)
+   )
> 
> long_summary_stats <-  summary |>
+   pivot_longer(cols = everything(), names_to = "Statistic", values_to = "Value") |>
+   separate(Statistic, into = c("Measure", "Variable"), sep = "_") |>
+   spread(key = Measure, value = Value) |>
+   select(Variable, Mean, Median, SD, Min, Max)
> 
> long_summary_stats$Variable <- c("Gender Gap", "GDP", "Income Attitude", "Political Attitude", "Violence Attitude")
> 
> kable(long_summary_stats,
+       format = "markdown",
+       digits = 2,
+       col.names = c("Variable", "Mean", "Median", "Standard Deviation", "Min", "Max"),
+       row.names = FALSE,
+       align = c('l', 'c', 'c', 'c', 'c', 'c'), 
+       )
> #| echo: false
> #| label: income_plot
> #| fig-cap: "Positive relationship in population and public art by ward number"
> #| warning: false 
> 
>   ggplot(data = data, aes(x = `Income attitude`, y = `Gender Gap`, label = `Code`)) +
+   geom_point() +
+   geom_text_repel(hjust=-0.5, vjust=0.5) +
+   geom_smooth(method = "lm", color = "red", se = TRUE) +
+   theme_minimal() +
+   labs(x = "Income Attitude", y = "Gender Gap") +
+   scale_colour_viridis_d()
Error in geom_text_repel(hjust = -0.5, vjust = 0.5) : 
  could not find function "geom_text_repel"
> #| include: false
> #| warning: false
> #| message: false
> 
> library(tidyverse)
> library(here)
> library(kableExtra)
> library(ggplot2)
> library(ggrepel)
> #| echo: false
> #| label: income_plot
> #| fig-cap: "Positive relationship in population and public art by ward number"
> #| warning: false 
> 
>   ggplot(data = data, aes(x = `Income attitude`, y = `Gender Gap`, label = `Code`)) +
+   geom_point() +
+   geom_text_repel(hjust=-0.5, vjust=0.5) +
+   geom_smooth(method = "lm", color = "red", se = TRUE) +
+   theme_minimal() +
+   labs(x = "Income Attitude", y = "Gender Gap") +
+   scale_colour_viridis_d()
Error in `geom_point()`:
! Problem while computing aesthetics.
ℹ Error occurred in the 1st layer.
Caused by error:
! object 'Income attitude' not found
Backtrace:
  1. base (local) `<fn>`(x)
  2. ggplot2:::print.ggplot(x)
  4. ggplot2:::ggplot_build.ggplot(x)
  5. ggplot2:::by_layer(...)
 12. ggplot2 (local) f(l = layers[[i]], d = data[[i]])
 13. l$compute_aesthetics(d, plot)
 14. ggplot2 (local) compute_aesthetics(..., self = self)
 15. base::lapply(aesthetics, eval_tidy, data = data, env = env)
 16. rlang (local) FUN(X[[i]], ...)
> #| echo: false
> #| label: income_plot
> #| fig-cap: "Positive relationship in population and public art by ward number"
> #| warning: false 
> 
>   ggplot(data = data, aes(x = `Income Attitude`, y = `Gender Gap`, label = `Code`)) +
+   geom_point() +
+   geom_text_repel(hjust=-0.5, vjust=0.5) +
+   geom_smooth(method = "lm", color = "red", se = TRUE) +
+   theme_minimal() +
+   labs(x = "Income Attitude", y = "Gender Gap") +
+   scale_colour_viridis_d()
> 
> #| echo: false
> #| label: income_plot
> #| fig-cap: "Positive relationship in negative attitude towards high income in women and the gender gap by country"
> #| warning: false 
> 
>   ggplot(data = data, aes(x = `Income Attitude`, y = `Gender Gap`, label = `Code`)) +
+   geom_point() +
+   geom_text_repel(hjust=-0.5, vjust=0.5) +
+   geom_smooth(method = "lm", color = "red", se = TRUE) +
+   theme_minimal() +
+   labs(x = "Attitudes on Women's Income", y = "Gender Gap") +
+   scale_colour_viridis_d()
> 
> #| echo: false
> #| label: income_plot
> #| fig-cap: "Positive relationship in negative attitude towards high income in women and the gender gap by country"
> #| warning: false 
> 
>   ggplot(data = data, aes(x = `Political Attitude`, y = `Gender Gap`, label = `Code`)) +
+   geom_point() +
+   geom_text_repel(hjust=-0.5, vjust=0.5) +
+   geom_smooth(method = "lm", color = "red", se = TRUE) +
+   theme_minimal() +
+   labs(x = "Attitudes on Women's Ability to be a Political Leader", y = "Gender Gap") +
+   scale_colour_viridis_d()
> 
> #| echo: false
> #| label: political_plot
> #| fig-cap: "Positive relationship in negative attitude towards women in political leadership and the gender gap by country"
> #| warning: false 
> 
>   ggplot(data = data, aes(x = `Political Attitude`, y = `Gender Gap`, label = `Code`)) +
+   geom_point() +
+   geom_text_repel(hjust=-0.5, vjust=0.5) +
+   geom_smooth(method = "lm", color = "red", se = TRUE) +
+   theme_minimal() +
+   labs(x = "Attitudes on Women's Ability to be a Political Leader", y = "Gender Gap") +
+   scale_colour_viridis_d()
> 
> #| echo: false
> #| label: violence_plot
> #| fig-cap: "Positive relationship in negative attitude towards women in political leadership and the gender gap by country"
> #| warning: false 
> 
>   ggplot(data = data, aes(x = `Violence Attitude`, y = `Gender Gap`, label = `Code`)) +
+   geom_point() +
+   geom_text_repel(hjust=-0.5, vjust=0.5) +
+   geom_smooth(method = "lm", color = "red", se = TRUE) +
+   theme_minimal() +
+   labs(x = "Attitudes Justifying Intimate-Partner Violence", y = "Gender Gap") +
+   scale_colour_viridis_d()
> 
> #| echo: false
> #| label: violence_plot
> #| fig-cap: "Positive relationship in attitudes justifying intimate partner violence towards women and the gender gap by country"
> #| warning: false 
> 
>   ggplot(data = data, aes(x = `Violence Attitude`, y = `Gender Gap`, label = `Code`)) +
+   geom_point() +
+   geom_text_repel(hjust=-0.5, vjust=0.5) +
+   geom_smooth(method = "lm", color = "red", se = TRUE) +
+   theme_minimal() +
+   labs(x = "Attitudes Justifying Intimate-Partner Violence", y = "Gender Gap") +
+   scale_colour_viridis_d()
> 
> #| echo: false
> #| label: gdp_plot
> #| fig-cap: "Positive relationship in attitudes justifying intimate partner violence towards women and the gender gap by country"
> #| warning: false 
> 
>   ggplot(data = data, aes(x = GDP, y = `Gender Gap`, label = `Code`)) +
+   geom_point() +
+   geom_text_repel(hjust=-0.5, vjust=0.5) +
+   geom_smooth(method = "lm", color = "red", se = TRUE) +
+   theme_minimal() +
+   labs(x = "GDP per capita", y = "Gender Gap") +
+   scale_colour_viridis_d()
> 
> #### Workspace setup ####
> library(tidyverse)
> library(rstanarm)
Loading required package: Rcpp
This is rstanarm version 2.32.1
- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!
- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.
- For execution on a local, multicore CPU with excess RAM we recommend calling
  options(mc.cores = parallel::detectCores())
> #### Read data ####
> analysis_data <- read_csv("data/analysis_data/analysis_data.csv")
Rows: 19 Columns: 3                                                                                                                          
── Column specification ──────────────────────────────────────────────────────────────────────────────────
Delimiter: ","
dbl (3): width, length, flying_time

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> ### Model data ####
> first_model <-
+   stan_glm(
+     formula = `Gender Gap` ~ `Income Attitude` + `Political Attitude` + `Violence Attitude` + GDP,
+     data = analysis_data,
+     family = gaussian(),
+     prior = normal(location = 0, scale = 2.5, autoscale = TRUE),
+     prior_intercept = normal(location = 0, scale = 2.5, autoscale = TRUE),
+     seed = 853
+   )
Error in eval(predvars, data, env) : object 'Gender Gap' not found
> #### Read data ####
> analysis_data <- read_csv("data/analysis_data/analysis_data.csv")
Rows: 19 Columns: 3                                                                                                                          
── Column specification ──────────────────────────────────────────────────────────────────────────────────
Delimiter: ","
dbl (3): width, length, flying_time

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> #### Read data ####
> analysis_data <- read_csv("data/analysis_data/analysis_data.csv")
Rows: 69 Columns: 7                                                                                                                          
── Column specification ──────────────────────────────────────────────────────────────────────────────────
Delimiter: ","
chr (2): Country, Code
dbl (5): Income Attitude, Violence Attitude, Political Attitude, GDP, Gender Gap

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> ### Model data ####
> first_model <-
+   stan_glm(
+     formula = `Gender Gap` ~ `Income Attitude` + `Political Attitude` + `Violence Attitude` + GDP,
+     data = analysis_data,
+     family = gaussian(),
+     prior = normal(location = 0, scale = 2.5, autoscale = TRUE),
+     prior_intercept = normal(location = 0, scale = 2.5, autoscale = TRUE),
+     seed = 853
+   )

SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.001572 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 15.72 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.039 seconds (Warm-up)
Chain 1:                0.044 seconds (Sampling)
Chain 1:                0.083 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 1.1e-05 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.038 seconds (Warm-up)
Chain 2:                0.043 seconds (Sampling)
Chain 2:                0.081 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 1.1e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.039 seconds (Warm-up)
Chain 3:                0.047 seconds (Sampling)
Chain 3:                0.086 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 7e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.042 seconds (Warm-up)
Chain 4:                0.039 seconds (Sampling)
Chain 4:                0.081 seconds (Total)
Chain 4: 
> #| label: tbl-cleaned_data
> #| tbl-cap: Sample of cleaned attitude, GDP, and gender gap data
> #| echo: false
> 
> data_path <- here("data", "analysis_data", "analysis_data.csv")
> data <- read_csv(data_path, show_col_types = FALSE)
> 
> data |>
+   select(-Code) |>
+   slice(1:5) |>
+   kable(align = c('c', 'c', 'c', 'c')) |>
+   kable_styling(latex_options = c("hold_position"))
>   
> #### Save model ####
> saveRDS(
+   first_model,
+   file = "models/first_model.rds"
+ )
> prior_summary(first_model)
Priors for model 'first_model' 
------
Intercept (after predictors centered)
  Specified prior:
    ~ normal(location = 0, scale = 2.5)
  Adjusted prior:
    ~ normal(location = 0, scale = 0.16)

Coefficients
  Specified prior:
    ~ normal(location = [0,0,0,...], scale = [2.5,2.5,2.5,...])
  Adjusted prior:
    ~ normal(location = [0,0,0,...], scale = [0.0099,0.0074,0.0089,...])

Auxiliary (sigma)
  Specified prior:
    ~ exponential(rate = 1)
  Adjusted prior:
    ~ exponential(rate = 16)
------
See help('prior_summary.stanreg') for more details
> ### Model data ####
> first_model <-
+   stan_glm(
+     formula = `Gender Gap` ~ `Income Attitude` + `Political Attitude` + `Violence Attitude` + GDP,
+     data = analysis_data,
+     family = gaussian(),
+     prior = normal(location = 0, scale = 2.5, autoscale = TRUE),
+     prior_intercept = normal(location = 0, scale = 2.5, autoscale = TRUE),
+     prior_aux = exponential(rate = 1, autoscale = TRUE),
+     seed = 853
+   )

SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 3.9e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.039 seconds (Warm-up)
Chain 1:                0.043 seconds (Sampling)
Chain 1:                0.082 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 1.3e-05 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.039 seconds (Warm-up)
Chain 2:                0.044 seconds (Sampling)
Chain 2:                0.083 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 1.2e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.039 seconds (Warm-up)
Chain 3:                0.043 seconds (Sampling)
Chain 3:                0.082 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 1.4e-05 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.044 seconds (Warm-up)
Chain 4:                0.038 seconds (Sampling)
Chain 4:                0.082 seconds (Total)
Chain 4: 
> prior_summary(first_model)
Priors for model 'first_model' 
------
Intercept (after predictors centered)
  Specified prior:
    ~ normal(location = 0, scale = 2.5)
  Adjusted prior:
    ~ normal(location = 0, scale = 0.16)

Coefficients
  Specified prior:
    ~ normal(location = [0,0,0,...], scale = [2.5,2.5,2.5,...])
  Adjusted prior:
    ~ normal(location = [0,0,0,...], scale = [0.0099,0.0074,0.0089,...])

Auxiliary (sigma)
  Specified prior:
    ~ exponential(rate = 1)
  Adjusted prior:
    ~ exponential(rate = 16) how do i display the other coefficients ?
ChatGPT
To display the coefficients from your model fit with stan_glm in R, you can use the summary function or directly access the coefficients with the coef function. Here's how you can do both:

Using the summary function: This function will give you a detailed summary of the model, including estimates of the coefficients, their standard errors, t-values, and p-values.
r
Copy code
summary(first_model)
Using the coef function: This function will provide you just the coefficients of the model.
r
Copy code
coef(first_model)
If you want to display these coefficients in a table format or include them in a report or publication, you can use the kable function from the knitr package to create a nicely formatted table. Here’s how you could do it:

r
Copy code
library(knitr)

# Extract coefficients
model_coefficients <- coef(summary(first_model))

# Create a data frame to display
coefficients_df <- data.frame(
  Estimate = model_coefficients[, "Estimate"],
  Std.Error = model_coefficients[, "Std. Error"],
  t.value = model_coefficients[, "t value"],
  Pr = model_coefficients[, "Pr(>|t|)"]
)

# Use kable to create a markdown table
kable(coefficients_df, caption = "Regression Coefficients", align = c('l', 'r', 'r', 'r', 'r'))
This approach will give you a clear display of the coefficients along with other relevant statistics, which is especially useful for interpretation and reporting in a more formal analysis setting.

User
im using the rstanarm package in R right now. i did prior_summary for a model i have. Coefficients
  Specified prior:
    ~ normal(location = [0,0,0,...], scale = [2.5,2.5,2.5,...])
  Adjusted prior:
    ~ normal(location = [0,0,0,...], scale = [0.0099,0.0074,0.0089,...]) how do i display all the adjusted priors?
ChatGPT
In rstanarm, the prior_summary function gives a concise summary of the priors used in the model, including the priors on the coefficients, the intercept, and the error term. However, it usually shows a condensed form for the priors on the coefficients, especially if many predictors are involved. To view all adjusted priors for each coefficient specifically, you might need to take a few additional steps to extract and display this information manually.

Here’s a general approach to display all adjusted priors for each coefficient using rstanarm:

Extract Prior Information: You'll have to create a custom function or a procedure to extract the adjusted scales directly from the prior_summary output or from the model object.
Display the Information: Format this information in a user-friendly way, possibly as a data frame or a table.
Here's a possible way to do it, assuming you have your model stored in first_model:

r
Copy code
# Load necessary library
library(rstanarm)

# Fit a model example (replace with your actual model fitting code)
first_model <- stan_glm(mpg ~ wt + qsec + am, data = mtcars, prior = normal(0, 2.5), seed = 123)

# Extracting and displaying prior details
prior_details <- prior_summary(first_model)

# Manually extract adjusted scales for each coefficient
# Note: You might need to adjust this depending on your specific model structure
coefficients_prior <- data.frame(
  Coefficient = names(prior_details$adjusted$scale),
  AdjustedScale = prior_details$adjusted$scale
)

# Print the coefficients and their adjusted prior scales
print(coefficients_prior)
This script does the following:

Fits a model (replace with your model details).
Uses prior_summary to extract prior settings.
Extracts the names of coefficients and their adjusted scales into a data frame.
Prints out the details.
If your model has many coefficients and the default prior_summary output is not showing all the details you need, this approach allows you to manually construct a detailed overview. Adjust the above code snippet to match the actual structure and scale of your model's coefficients.

User
what is 6.62e-06 as a decimal?
ChatGPT
The value 
6.62
×
1
0
−
6
6.62×10 
−6
  as a decimal is 
0.00000662
0.00000662. ​​

User
Uploaded image
how is this Posterior prediction check
ChatGPT
This plot appears to be showing the results of a posterior predictive check for a Bayesian model. This type of plot is commonly used to assess the fit of a model by comparing the observed data (often denoted as y) with data simulated from the model's posterior distribution (often denoted as y_rep).

Here's what you can infer from the plot:

The dark line labeled y represents the observed data's distribution.
The lighter lines labeled y_rep represent the distributions of the predicted data generated from the model's posterior.
The large number of light lines suggests that multiple sets of predicted data were generated, which is typical for posterior predictive checks. This allows you to see the variability in the predictions that the model produces.
The overlap between the dark line (y) and the lighter lines (y_rep) gives an indication of how well the model predictions correspond to the observed data.
Ideally, you want the predictions to cover the observed data well. If the dark line falls comfortably within the spread of the lighter lines, this suggests the model is doing a reasonable job of capturing the data's distribution. However, if there are systematic deviations—such as the observed data consistently falling in the tails of the predicted distributions—it might suggest the model is not adequately capturing the structure of the data.

This type of visual check is qualitative; it's about looking for major discrepancies rather than exact matches. For a more quantitative assessment, you could calculate posterior predictive p-values or use other discrepancy measures.

User
phase change of a wave from n1 to n2
ChatGPT
When a wave transitions from one medium to another with different refractive indices, a phase change may occur at the boundary. The refractive index of a medium is a measure of how much it reduces the speed of light compared to the speed of light in a vacuum.

Let's denote 
�
1
n 
1
​
  as the refractive index of the first medium and 
�
2
n 
2
​
  as the refractive index of the second medium.

When a wave, such as light, goes from a medium with a lower refractive index to a medium with a higher refractive index (i.e., 
�
1
<
�
2
n 
1
​
 <n 
2
​
 ), it slows down and the wavefront bends towards the normal. This is described by Snell's Law. If the wave is a light wave and is reflecting off the boundary as well, the phase of the reflected wave is inverted, which corresponds to a phase change of 180 degrees or 
�
π radians.

Conversely, if the wave goes from a medium with a higher refractive index to one with a lower refractive index (i.e., 
�
1
>
�
2
n 
1
​
 >n 
2
​
 ), it speeds up and the wavefront bends away from the normal. For the reflected wave, there is no phase change upon reflection in this case.

For the transmitted wave (the part that continues through the new medium), the phase change is not typically a complete 180 degrees, but there is a continuous change in phase due to the change in the wave's speed. The exact phase change depends on the path length difference in the two media, which can be related to the wavelength difference due to the change in speed.

To calculate the actual phase change for the transmitted wave, you would need to know more specifics about the wave, such as the wavelength in both media, the angle of incidence, and the thickness of the second medium if it is finite.


